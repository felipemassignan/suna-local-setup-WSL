# Suna Local Setup

A complete, 100% local implementation of the [Suna AI agent framework](https://github.com/kortix-ai/suna) that runs entirely on your own hardware without any external API dependencies. This system replaces all cloud services with local alternatives, centered around a Mistral 7B model served via llama.cpp.

## Features

- **Completely offline operation** - no external API dependencies whatsoever
- Uses Mistral 7B Instruct model via llama.cpp for local AI capabilities
- Replaces all cloud services with local alternatives:
  - Local LLM instead of OpenAI
  - Local FAISS vector store for document retrieval
  - Local mock search instead of Tavily
  - Local mock image generation
  - Local authentication instead of Supabase
  - Local file storage instead of cloud storage
- Optimized for CPU-only operation with minimal resource usage
- Bypasses authentication and database requirements in LOCAL mode
- Includes systemd service files for all components
- Provides scripts for easy installation and management

# Suna Local Setup - WSL2 Edition

Fork otimizado do [suna-local-setup](https://github.com/88atman77/suna-local-setup) para ambiente WSL2 com acesso via navegador Windows.

## ðŸŽ¯ **CaracterÃ­sticas**

- âœ… InstalaÃ§Ã£o automatizada no WSL2
- âœ… ConfiguraÃ§Ã£o automÃ¡tica de rede
- âœ… Acesso via navegador Windows
- âœ… Scripts de monitoramento
- âœ… ConfiguraÃ§Ã£o automÃ¡tica de firewall
- âœ… Backup automatizado

## âš¡ **InstalaÃ§Ã£o RÃ¡pida**

```bash
# 1. Clone este repositÃ³rio no WSL2
git clone https://github.com/felipemassignan/suna-wsl2-setup.git
cd suna-wsl2-setup

# 2. Execute a instalaÃ§Ã£o automatizada
chmod +x *.sh
./install_wsl2.sh

# 3. Configure o Windows (PowerShell como Admin)
cd windows
.\configure_firewall.ps1
.\update_hosts.ps1
## Troubleshooting

## **Melhorias PrioritÃ¡rias a Implementar**

### **Alta Prioridade:**
1. âœ… Melhorar `install_wsl2.sh` com verificaÃ§Ãµes robustas
2. âœ… Criar `scripts/common.sh` com funÃ§Ãµes compartilhadas  
3. âœ… Adicionar documentaÃ§Ã£o detalhada
4. âœ… Melhorar tratamento de erros nos scripts
5. âœ… Adicionar verificaÃ§Ã£o de recursos do sistema

### **MÃ©dia Prioridade:**
1. ðŸ”„ Criar script de atualizaÃ§Ã£o automÃ¡tica
2. ðŸ”„ Implementar sistema de logs estruturado
3. ðŸ”„ Adicionar testes automatizados
4. ðŸ”„ Criar dashboard web de monitoramento
5. ðŸ”„ Implementar backup incremental

### **Baixa Prioridade:**
1. ðŸ“‹ Adicionar suporte a outros modelos LLM
2. ðŸ“‹ Criar interface de configuraÃ§Ã£o web
3. ðŸ“‹ Implementar mÃ©tricas de performance
4. ðŸ“‹ Adicionar suporte a Docker Compose
5. ðŸ“‹ Criar instalador GUI para Windows

## ðŸŽ¯ **PrÃ³ximos Passos Recomendados**

1. **Implementar as melhorias de alta prioridade**
2. **Testar instalaÃ§Ã£o em ambiente limpo**
3. **Criar CI/CD para testes automatizados**
4. **Documentar casos de uso especÃ­ficos**
5. **Criar vÃ­deo tutorial de instalaÃ§Ã£o**

### High Memory Usage

If the system is running out of memory:
1. Reduce the Redis memory limit
2. Use a more quantized model (Q2_K instead of Q4_K)
3. Reduce the context size in llama.cpp server

### Slow Response Times

If responses are too slow:
1. Adjust the number of threads based on your CPU
2. Consider using a smaller model if necessary

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgements

- [Suna AI](https://github.com/kortix-ai/suna) - The original AI agent framework
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - Efficient inference of LLaMA models
- [Mistral AI](https://mistral.ai/) - Creators of the Mistral 7B model
- [FAISS](https://github.com/facebookresearch/faiss) - Vector similarity search library
